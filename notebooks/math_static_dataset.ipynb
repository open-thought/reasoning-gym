{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ad847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "import json\n",
    "from reasoning_gym.utils import SYSTEM_PROMPTS\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdade008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22000 samples from 2 evals\n"
     ]
    }
   ],
   "source": [
    "# Load all samples\n",
    "evals = [\n",
    "    Path(\"./eval/math-traces/x-ai_grok-3-mini-beta_easy\"),\n",
    "    Path(\"./eval/math-traces/x-ai_grok-3-mini-beta_medium\"),\n",
    "]\n",
    "\n",
    "samples = [] \n",
    "\n",
    "for eval in evals:\n",
    "    for category in eval.iterdir():\n",
    "        if not category.is_dir():\n",
    "            continue\n",
    "        for dataset in category.iterdir():\n",
    "            with open(dataset, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            for sample in data[\"results\"]:\n",
    "                samples.append({\n",
    "                    \"question\": sample[\"question\"],\n",
    "                    \"answer\": sample[\"best_model_answer\"],\n",
    "                    \"final_model_answer\": sample[\"best_model_answer\"],\n",
    "                    \"full_model_answer\": sample[\"best_full_model_response\"],\n",
    "                    \"score\": sample[\"best_score\"],\n",
    "                    \"metadata\": sample[\"metadata\"],\n",
    "                })\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples from {len(evals)} evals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90e162dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered samples: 16937\n"
     ]
    }
   ],
   "source": [
    "# Filter out samples with score less than a given threshold\n",
    "THRESHOLD = 0.9\n",
    "filtered_samples = [sample for sample in samples if sample[\"score\"] >= THRESHOLD]\n",
    "print(f\"Filtered samples: {len(filtered_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02e8d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered samples for RL training saved to training/data/train_rl.jsonl\n",
      "Filtered samples for SFT saved to training/data/train_sft.json\n"
     ]
    }
   ],
   "source": [
    "# Save the filtered samples for RL training\n",
    "output_path_rl = Path(\"./training/data/train_rl.jsonl\")\n",
    "output_path_rl.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(output_path_rl, \"w\") as f:\n",
    "    for sample in filtered_samples:\n",
    "        f.write(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"question\": sample[\"question\"],\n",
    "                    \"answer\": sample[\"answer\"],\n",
    "                    \"metadata\": sample[\"metadata\"],\n",
    "                }\n",
    "            ) + \"\\n\"\n",
    "        )\n",
    "print(f\"Filtered samples for RL training saved to {output_path_rl}\")\n",
    "\n",
    "\n",
    "# Save the filtered samples for Supervised Fine-Tuning (SFT)\n",
    "output_path_sft = Path(\"./training/data/train_sft.json\")\n",
    "output_path_sft.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sft_samples = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPTS[\"DeepSeekZero\"]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sample[\"question\"]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": sample[\"full_model_answer\"]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    for sample in filtered_samples\n",
    "]\n",
    "\n",
    "with open(output_path_sft, \"w\") as f:\n",
    "    json.dump(sft_samples, f, indent=4)\n",
    "print(f\"Filtered samples for SFT saved to {output_path_sft}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b728c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Literal, Optional\n",
    "# from enum import Enum\n",
    "\n",
    "# import numpy as np\n",
    "# from torch.utils.data import Dataset\n",
    "# from transformers import PreTrainedTokenizer\n",
    "\n",
    "# from reasoning_gym.coaching.experiment import Experiment\n",
    "# from reasoning_gym.dataset import ProceduralDataset\n",
    "# from reasoning_gym.factory import get_score_answer_fn\n",
    "\n",
    "\n",
    "# class DatasetType(Enum):\n",
    "#     STATIC = \"static\" # static dataset stored on disk\n",
    "#     PROCEDURAL = \"procedural\" # procedural dataset generated on-the-fly\n",
    "#     EXPERIMENT = \"experiment\" # procedural dataset within an experiment\n",
    "\n",
    "\n",
    "# class ReasoningGymDataset(Dataset):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         tokenizer: PreTrainedTokenizer,\n",
    "#         static_dataset: list[dict] = None,\n",
    "#         procedural_dataset: Optional[ProceduralDataset] = None,\n",
    "#         experiment: Optional[Experiment] = None,\n",
    "#         developer_prompt: Optional[str] = None,\n",
    "#         developer_role: str = \"system\",\n",
    "#         max_prompt_length: int = 2048,\n",
    "#         truncation: str = \"error\",  ##  ['left', 'right', 'error']\n",
    "#     ):\n",
    "#         assert static_dataset or procedural_dataset or experiment, \"One of `static_dataset`, `procedural_dataset` or `experiment` must be provided\"\n",
    "#         assert sum(x is not None for x in [static_dataset, procedural_dataset, experiment]) == 1, \"Exactly one of `static_dataset`, `procedural_dataset` or `experiment` must be provided\"\n",
    "#         if static_dataset:\n",
    "#             self.dataset_type = DatasetType.STATIC\n",
    "#         if procedural_dataset:\n",
    "#             self.dataset_type = DatasetType.PROCEDURAL\n",
    "#         if experiment:\n",
    "#             self.dataset_type = DatasetType.EXPERIMENT\n",
    "\n",
    "#         self.data = static_dataset or procedural_dataset or experiment.composite\n",
    "#         self.experiment = experiment\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.developer_prompt = developer_prompt\n",
    "#         self.developer_role = developer_role\n",
    "#         self.max_prompt_length = max_prompt_length\n",
    "#         self.truncation = truncation\n",
    "\n",
    "#     def __len__(self) -> int:\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         row_dict = self.data[index].copy()\n",
    "#         q = row_dict[\"question\"]\n",
    "\n",
    "#         chat = []\n",
    "#         if self.developer_prompt is not None:\n",
    "#             chat.append({\"role\": self.developer_role, \"content\": self.developer_prompt})\n",
    "#         chat.append({\"role\": \"user\", \"content\": q})\n",
    "\n",
    "#         prompt = self.tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "#         row_dict[\"data_source\"] = \"reasoning_gym\"\n",
    "#         row_dict[\"raw_prompt_ids\"] = self.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "#         row_dict[\"raw_prompt\"] = chat\n",
    "#         row_dict[\"index\"] = index\n",
    "#         return row_dict\n",
    "\n",
    "#     def score_answer(self, answer: str, index: int) -> float:\n",
    "#         \"\"\"Score the answer using the underlying experiment's scorer.\"\"\"\n",
    "#         entry = self.data[index]\n",
    "#         if self.dataset_type == DatasetType.EXPERIMENT:\n",
    "#             return self.experiment.score_answer_with_id(answer, entry[\"metadata\"][\"entry_id\"])\n",
    "#         if self.dataset_type == DatasetType.PROCEDURAL:\n",
    "#             return self.data.score_answer(answer, entry=entry)\n",
    "#         if self.dataset_type == DatasetType.STATIC:\n",
    "#             score_fn = get_score_answer_fn(entry[\"metadata\"][\"source_dataset\"])\n",
    "#             return score_fn(answer, entry)\n",
    "#         raise ValueError(\"No valid scoring method available\")\n",
    "\n",
    "#     def update_experiment_difficulty(self, dataset_name: str, method: Literal[\"increment\", \"decrement\"]):\n",
    "#         \"\"\"Update the difficulty of the underlying dataset.\"\"\"\n",
    "#         if self.experiment is None:\n",
    "#             raise ValueError(\"Cannot update difficulty: dataset is not a CurriculumExperiment\")\n",
    "#         if method not in [\"increment\", \"decrement\"]:\n",
    "#             raise ValueError(\"Invalid method: must be 'increment' or 'decrement'\")\n",
    "#         self.experiment.score_board.clear(dataset_name)\n",
    "#         self.experiment.update_difficulty(dataset_name, method)\n",
    "#         self.data = self.experiment.composite\n",
    "#         return True\n",
    "\n",
    "#     def aggregate(self, last_n: Optional[int] = None):\n",
    "#         \"\"\"Aggregate scores from the underlying experiment\"\"\"\n",
    "#         if self.experiment is None:\n",
    "#             raise ValueError(\"Cannot aggregate scores: dataset is not a CurriculumExperiment\")\n",
    "\n",
    "#         results = self.experiment.score_board.aggregate(last_n=last_n)\n",
    "#         output_results = {}\n",
    "\n",
    "#         for key, value in results.items():\n",
    "#             output_results[key] = {}\n",
    "#             scores = value.scores\n",
    "#             first_key = list(scores.keys())[0]\n",
    "#             output_results[key][\"results\"] = np.mean(scores[first_key])\n",
    "#             output_results[key][\"total_samples\"] = value.total_scores\n",
    "#         return output_results\n",
    "\n",
    "\n",
    "# def make_dataset(\n",
    "#     tokenizer,\n",
    "#     data_source: list[dict] | Experiment | ProceduralDataset,\n",
    "#     developer_prompt: str,\n",
    "#     max_prompt_length: int = 2048,\n",
    "# ) -> ReasoningGymDataset:\n",
    "#     \"\"\"\n",
    "#     Create ReasoningGymDataset object using either a ProceduralDataset or Experiment as the underlying data source.\n",
    "#     \"\"\"\n",
    "#     if isinstance(data_source, Experiment):\n",
    "#         return ReasoningGymDataset(\n",
    "#             tokenizer=tokenizer,\n",
    "#             experiment=data_source,\n",
    "#             developer_prompt=developer_prompt,\n",
    "#             developer_role=\"system\",\n",
    "#             max_prompt_length=max_prompt_length,\n",
    "#             truncation=\"error\",\n",
    "#         )\n",
    "#     elif isinstance(data_source, ProceduralDataset):\n",
    "#         return ReasoningGymDataset(\n",
    "#             tokenizer=tokenizer,\n",
    "#             procedural_dataset=data_source,\n",
    "#             developer_prompt=developer_prompt,\n",
    "#             developer_role=\"system\",\n",
    "#             max_prompt_length=max_prompt_length,\n",
    "#             truncation=\"error\",\n",
    "#         )\n",
    "#     elif isinstance(data_source, list):\n",
    "#         return ReasoningGymDataset(\n",
    "#             tokenizer=tokenizer,\n",
    "#             static_dataset=data_source,\n",
    "#             developer_prompt=developer_prompt,\n",
    "#             developer_role=\"system\",\n",
    "#             max_prompt_length=max_prompt_length,\n",
    "#             truncation=\"error\",\n",
    "#         )\n",
    "#     else:\n",
    "#         raise ValueError(\"data_source must be either a Dataset, ProceduralDataset, or Experiment\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "# train_data_path = Path(\"./training/data/train_rl.jsonl\")\n",
    "# with open(train_data_path, \"r\") as f:\n",
    "#     train_data_source = [json.loads(line) for line in f.readlines() if line.strip()]\n",
    "\n",
    "# train_dataset = make_dataset(\n",
    "#     tokenizer, train_data_source, SYSTEM_PROMPTS[\"DeepSeekZero\"], max_prompt_length=2048,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cdee1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
